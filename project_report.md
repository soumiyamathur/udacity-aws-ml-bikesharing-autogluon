# PROJECT- Predict Bike Sharing Demand with AutoGluon
## Overview
    Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations
    throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are
    over 500 bike-sharing programs around the world. The data generated by these systems makes them attractive for researchers because the duration of travel, departure
    location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility
    in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital
    Bikeshare program in Washington, D.C.
    
## Dataset

    1. datetime - hourly date + timestamp  
    2. season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
    3. holiday - whether the day is considered a holiday
    4. workingday - whether the day is neither a weekend nor holiday
    5. weather -
            1: Clear, Few clouds, Partly cloudy, Partly cloudy
            2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
            3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
            4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 



    6. temp - temperature in Celsius
    7. atemp - "feels like" temperature in Celsius
    8. humidity - relative humidity
    9. windspeed - wind speed
    10. casual - number of non-registered user rentals initiated
    11. registered - number of registered user rentals initiated
    12. count - number of total rentals



## Initial model using TabularPredictor

1. We trained an initial model with Autogluon TabularPredictor, ignored the columns "casual" and "registered" as they are not part of Test dataset as well. 
2. Preset selected for model choice is "best_quality", this preset provide best predictive accuracy with little consideration to inference time or disk usage. 
3. TIme_Limit parameter value is set to 600 seconds is the time given to a particular model to train.


![My Image](images/initial-model-leaderboard.png)

4. WeightedEnseble scored topped the initial model leaderboard.
5. After submitting the initial model, we got a score of 1.79095 



## Exploratory Data Analysis


### Train dataset description

![My Image](images/train-describe.png)

### Train dataset correlation matrix

![My Image](images/train-corr.png)

### Train dataset correlation heatmap

![My Image](images/train-heatmap.png)

### Train dataset pairplot

![My Image](images/train-pairplot.png)


    1. Training dataset has 10,886 rows of data and 12 features
    2. Mean temp and temp values are 20.2 Celsius and 23.6 Celsius respectively
    3. Temp varies between 0-40 Celsius
    4. Mean relative humidity is 61 and valor between 0-100
    5. Mean windspeed is 12.7 and varies between 0 to 56.9
    6. All the features have no null values
    7. There are four unique values representing seasons for the season feature. We will convert this to category type as that is a better representation of that feature.
    8. There are four unique values representing weather feature. We will convert this to category type as that is a better representation of that feature.
    9. Holiday and Workingday have binary values. We will convert both of these features to boolean type.
    10. Based on pair plot, Registered bikes are rented more and are highly correlated.
    11. In medium Temperature, and humidiy bikes are rented more rather than when itâ€™s too hot or cold or too humid.
    

## Data Preprocessing and New Features

    1. Based on EDA for festures "season" and "weather", we found that they have 4 unique values.
    2. We converted the type for these 2 features as category instead of int, so that same can be indicaed to the model
    3. Holiday and WOrkingday features also have 2 unique values each (0 and 1), since these are binary values, we converted these feature types to boolean.
    4. We created new features in train and test dataset-  day, date and year from datetime column.
    5. Since the new features were derived from datetime column, we dropped that feature from train and test dataset.


## Retrain model with new features

    1. We trained next model with new features (day, month, year) with Autogluon TabularPredictor, ignored the columns "casual" and "registered" as they are not part of Test dataset as well. 
    2. Preset selected for model choice is "best_quality", this preset provide best predictive accuracy with little consideration to inference time or disk usage. 
    3. TIme_Limit parameter value is set to 600 seconds is the time given to a particular model to train.

![My Image](images/newfeatures-model-leaderboard.png)

    4. WeightedEnseble still scored topped the model leaderboard with new features as well.
    5. After submitting the model trained with new features, we got a score of 1.33117 
      

## Retrain model with hyperparameters (num_bag_folds=5, num_bag_sets=1, num_stack_levels=1)

    1. We trained next model with new features (day, month, year) with Autogluon TabularPredictor, ignored the columns "casual" and "registered" as they are not part of Test dataset as well. 
    2. Preset selected for model choice is "best_quality", this preset provide best predictive accuracy with little consideration to inference time or disk usage. 
    3. TIme_Limit parameter value is set to 600 seconds is the time given to a particular model to train.
    4. Hyperparameters selected num_bag_folds=5, num_bag_sets=1, num_stack_levels=1

![My Image](images/hyperparam-model-leaderboard.png)

    4. WeightedEnseble still scored topped the model leaderboard with new features & hyperparameters as well.
    5. After submitting the model trained with new features and hyperparameters, we got a score of 1.32531 
    

## Retrain model with hyperparameters (num_bag_folds=10, num_bag_sets=2, num_stack_levels=2)

    1. For the second iteration with hyperparameters, following params were changed:
        
        num_bag_folds(10)= Number of folds used for bagging of models, to avoid overfitting, keeping the value to 10
        num_bag_sets(2)= Number of repeats of kfold bagging to perform. Total number of models trained during bagging = num_bag_folds * num_bag_sets. 
        num_stack_levels (2)= Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of num_stack_levels+1
        
![My Image](images/hyperparamv2-model-leaderboard.png)

    2. WeightedEnseble still scored topped the model leaderboard with new features and hyperparamters updated values as well.
    3. After submitting the model trained with new features and hyperparameters, we got a score of 1.32230 



## Retrain model with hyperparameters (num_bag_folds=5, num_bag_sets=1, num_stack_levels=1,hyperparameters=hyperparameters, hyperparameter_tune_kwargs='auto')

    1. For the third iteration with hyperparameters, following params were changed:
        
        num_bag_folds(5)= Number of folds used for bagging of models, to avoid overfitting, keeping the value to 10
        num_bag_sets(1)= Number of repeats of kfold bagging to perform. Total number of models trained during bagging = num_bag_folds * num_bag_sets. 
        num_stack_levels (1)= Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of num_stack_levels+1
    2. Used hyperparameter argument for model fitting and updated few learning rate default values for XGBoost and GBM models with new values with  
    3. Also used argument for hyperparameter tuning strategy set to "auto" to use random presets for hyperparameter default values and space.
        
![My Image](images/hyperparamv3-model-leaderboard.png)

    4. WeightedEnseble still scored topped the model leaderboard with new features and hyperparamters updated values as well.
    5. After submitting the model trained with new features and hyperparameters, we got a score of 1.32147 



## Model Score visualization

### model-score-table

![My Image](images/model-score-table.png)

### model-score-visualization

![My Image](images/model-score-val-viz.png)

### model-kaggle-score-visualization

![My Image](images/model-kaggle-score-viz.png)

    1. WeightedEnsemble_L3 model is on top in the model leaderboard for all four runs.
    2. Model score improved after adding new features (month, day, year) derived from datetime.
    3. Hyperparameters tuning only improved the score slightly.
    4. Utilizing hyperparameter tuning strategy argument made the score a bit better and reach to kaggle score 1.32147
    5. The initial submitted model scored the lowest due to the fact that we didn't do any feature enginnering for the training dataset. 
    6. Feature Enginnering did the most to gain considerably with model scoring, we can further deep dive and pick one feature out of "season vs weather" or "workingday vs holiday", this might help in improving the score as these features seems to be redundant and might be overfitting the models.
    7. Utilizing hyper parameters added value, but updating presets for hyperparamter (model learning rate) didn't add much of value.
    8. For future run, I would like to deep dive with Feature Engineering and pick some feature over other in training data than tweaking the hyperparameter presents.
    9. We can also look at data and check for skewness, highly skewed data can be removed from the training set to avoid model overfitting.
    